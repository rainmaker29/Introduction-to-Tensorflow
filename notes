Layers:
	
	Dense layer - Each node of current layer is fully connected to every node of previous and next layer.
	Flatten layer - A layer in which a 2D image is converted into a vector whose each value is a node of this layer

Learning rate: Step size to update weights during tuning/gradient descent

ReLu :  An Activation function
	
ReLu o/p : -ve : 0 ; +ve : +ve value

ReLu is used so that neural networks can solve non linear problems too

If NN is used for classification, it usually ends up with softmax being the activation function of the last dense layer

Another activation function for last layer in classification is Sigmoid,whose loss function must be binary_crossentropy

Check out loss functions

Image Augumentation : rotate,zoom etc operations on image to increase variance in training data


Dropout : turns off few neurons randomly so that every neuron contributes to prediction hence proper distribution of weights,lesser chances of
overfitting


