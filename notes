Layers:
	
	Dense layer - Each node of current layer is fully connected to every node of previous and next layer.
	Flatten layer - A layer in which a 2D image is converted into a vector whose each value is a node of this layer

Learning rate: Step size to update weights during tuning/gradient descent

ReLu :  An Activation function
	
ReLu o/p : -ve : 0 ; +ve : +ve value

ReLu is used so that neural networks can solve non linear problems too

If NN is used for classification, it usually ends up with softmax being the activation function of the last dense layer

Another activation function for last layer in classification is Sigmoid,whose loss function must be binary_crossentropy

Check out loss functions

Image Augumentation : rotate,zoom etc operations on image to increase variance in training data


Dropout : turns off few neurons randomly so that every neuron contributes to prediction hence proper distribution of weights,lesser chances of
overfitting

Techniques to avoid overfitting:

1.Image Augumentation
2.Dropout Layers
3.Early Stopping
4. Regularization - L1,L2..
5. Simplifying model
6. Feature Engineering

Transfer Learning: A technique that reuses a model that was created by machine learning experts and that has already been trained on a large dataset. When performing transfer learning we must always change the last layer of the pre-trained model so that it has the same number of classes that we have in the dataset we are working with. 

     

Freezing Parameters: Setting the variables of a pre-trained model to non-trainable. By freezing the parameters, we will ensure that only the variables of the last classification layer get trained, while the variables from the other layers of the pre-trained model are kept the same.


MobileNet: A state-of-the-art convolutional neural network developed by Google that uses a very efficient neural network architecture that minimizes the amount of memory and computational resources needed, while maintaining a high level of accuracy. MobileNet is ideal for mobile devices that have limited memory and computational resources.

